{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb2c3537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/shuirh/project/research/invariant_learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05bc27d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76e07f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'trainer' in globals():\n",
    "    importlib.reload(trainer)\n",
    "else:\n",
    "    import modules.trainer as trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60b521fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23ad52f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "class MNIST_DS(Dataset):\n",
    "    def __init__(self, vd):\n",
    "        self.vd = vd\n",
    "    def __len__(self):\n",
    "        return len(self.vd)\n",
    "    def __getitem__(self, idx):\n",
    "        x,y = self.vd[idx]\n",
    "        return {\n",
    "            'x': x,\n",
    "            'labels': y\n",
    "        }\n",
    "tr_ds = MNIST_DS(training_data)\n",
    "te_ds = MNIST_DS(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7be6ec3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, labels = None):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        outputs = {'logits': logits}\n",
    "        if labels is not None:\n",
    "            loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "            outputs['loss'] = loss\n",
    "        return outputs\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e53aa793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(all_logits, all_labels):\n",
    "    \"\"\"\n",
    "    Metric should start with `eval_`\n",
    "    \"\"\"\n",
    "    all_preds = np.argmax(all_logits, axis = -1)\n",
    "    num_samples = len(all_preds)\n",
    "    acc = (all_preds == all_labels).sum() / num_samples\n",
    "    metrics = {\n",
    "        'eval_acc': acc\n",
    "    }\n",
    "    return metrics\n",
    "args1 = trainer.SimpleTrainingArguments(\n",
    "    output_dir = './trainer_output',\n",
    "    eval_epochs = 1,\n",
    "    num_train_epochs = 3,\n",
    "    learning_rate = 1e-2,\n",
    "    per_device_train_batch_size = 64\n",
    ")\n",
    "trainer1 = trainer.BasicTrainer(\n",
    "    model,\n",
    "    args = args1,\n",
    "    train_dataset = tr_ds,\n",
    "    eval_dataset = te_ds,\n",
    "    compute_metrics = compute_accuracy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c33e6d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Total optimization steps = 2814\n",
      "  0%|                                                  | 0/2814 [00:00<?, ?it/s]/home/shuirh/lib/anaconda3/envs/st/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "  3%|█▍                                       | 97/2814 [00:02<00:43, 62.45it/s]{'loss': 0.6206926579773426, 'learning_rate': 0.0009644633972992182, 'step': 100, 'epoch': 0.11}\n",
      "  7%|██▊                                     | 200/2814 [00:04<00:40, 65.07it/s]{'loss': 0.3264257598668337, 'learning_rate': 0.0009289267945984365, 'step': 200, 'epoch': 0.21}\n",
      " 10%|████▏                                   | 295/2814 [00:05<00:33, 74.89it/s]{'loss': 0.23941356915980577, 'learning_rate': 0.0008933901918976545, 'step': 300, 'epoch': 0.32}\n",
      " 14%|█████▌                                  | 395/2814 [00:06<00:33, 71.44it/s]{'loss': 0.19263759993016719, 'learning_rate': 0.0008578535891968728, 'step': 400, 'epoch': 0.43}\n",
      " 18%|███████                                 | 495/2814 [00:08<00:32, 70.87it/s]{'loss': 0.17925576273351906, 'learning_rate': 0.000822316986496091, 'step': 500, 'epoch': 0.53}\n",
      " 21%|████████▍                               | 597/2814 [00:09<00:33, 67.06it/s]{'loss': 0.16334910530596972, 'learning_rate': 0.0007867803837953092, 'step': 600, 'epoch': 0.64}\n",
      " 25%|█████████▊                              | 694/2814 [00:11<00:31, 67.04it/s]{'loss': 0.15237035401165486, 'learning_rate': 0.0007512437810945275, 'step': 700, 'epoch': 0.75}\n",
      " 28%|███████████▎                            | 798/2814 [00:12<00:23, 84.42it/s]{'loss': 0.15777422742918135, 'learning_rate': 0.0007157071783937455, 'step': 800, 'epoch': 0.85}\n",
      " 32%|████████████▊                           | 900/2814 [00:13<00:21, 87.20it/s]{'loss': 0.11353815532289445, 'learning_rate': 0.0006801705756929638, 'step': 900, 'epoch': 0.96}\n",
      " 33%|█████████████▎                          | 937/2814 [00:14<00:26, 71.10it/s]***** Running evaluation *****\n",
      "  Num examples = 1250\n",
      "  Batch size = 8\n",
      "{'eval_acc': 0.9621, 'step': 938, 'epoch': 1.0}\n",
      "Saving model checkpoint to ./trainer_output/checkpoint-938\n",
      " 34%|█████████████▍                          | 945/2814 [00:19<05:54,  5.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_acc': 0.9621}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|██████████████▏                         | 997/2814 [00:20<00:58, 30.93it/s]{'loss': 0.09905611813766882, 'learning_rate': 0.000644633972992182, 'step': 1000, 'epoch': 1.07}\n",
      " 39%|███████████████▏                       | 1099/2814 [00:21<00:23, 73.07it/s]{'loss': 0.10436855559237301, 'learning_rate': 0.0006090973702914001, 'step': 1100, 'epoch': 1.17}\n",
      " 42%|████████████████▌                      | 1193/2814 [00:23<00:27, 59.83it/s]{'loss': 0.08180068640038371, 'learning_rate': 0.0005735607675906183, 'step': 1200, 'epoch': 1.28}\n",
      " 46%|██████████████████                     | 1299/2814 [00:25<00:25, 60.26it/s]{'loss': 0.080203139600344, 'learning_rate': 0.0005380241648898365, 'step': 1300, 'epoch': 1.39}\n",
      " 50%|███████████████████▎                   | 1396/2814 [00:26<00:21, 65.93it/s]{'loss': 0.07547680045012385, 'learning_rate': 0.0005024875621890548, 'step': 1400, 'epoch': 1.49}\n",
      " 53%|████████████████████▋                  | 1494/2814 [00:27<00:19, 66.34it/s]{'loss': 0.0712768396968022, 'learning_rate': 0.00046695095948827295, 'step': 1500, 'epoch': 1.6}\n",
      " 57%|██████████████████████▏                | 1600/2814 [00:29<00:18, 66.47it/s]{'loss': 0.07259927229955793, 'learning_rate': 0.0004314143567874911, 'step': 1600, 'epoch': 1.71}\n",
      " 60%|███████████████████████▌               | 1696/2814 [00:30<00:14, 74.77it/s]{'loss': 0.0719634905597195, 'learning_rate': 0.00039587775408670935, 'step': 1700, 'epoch': 1.81}\n",
      " 64%|████████████████████████▊              | 1794/2814 [00:32<00:14, 70.58it/s]{'loss': 0.06276126603595912, 'learning_rate': 0.0003603411513859275, 'step': 1800, 'epoch': 1.92}\n",
      " 67%|█████████████████████████▉             | 1874/2814 [00:33<00:14, 66.23it/s]***** Running evaluation *****\n",
      "  Num examples = 1250\n",
      "  Batch size = 8\n",
      "{'eval_acc': 0.9737, 'step': 1876, 'epoch': 2.0}\n",
      "Saving model checkpoint to ./trainer_output/checkpoint-1876\n",
      " 67%|██████████████████████████▏            | 1887/2814 [00:38<02:44,  5.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_acc': 0.9737}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████████████████████████▎            | 1895/2814 [00:39<01:53,  8.07it/s]{'loss': 0.054000143457669764, 'learning_rate': 0.0003248045486851457, 'step': 1900, 'epoch': 2.03}\n",
      " 71%|███████████████████████████▋           | 1998/2814 [00:40<00:09, 88.63it/s]{'loss': 0.04194109082454815, 'learning_rate': 0.0002892679459843639, 'step': 2000, 'epoch': 2.13}\n",
      " 75%|█████████████████████████████          | 2099/2814 [00:41<00:11, 64.16it/s]{'loss': 0.04782524004112929, 'learning_rate': 0.0002537313432835821, 'step': 2100, 'epoch': 2.24}\n",
      " 78%|██████████████████████████████▍        | 2194/2814 [00:43<00:09, 64.00it/s]{'loss': 0.03669497192371637, 'learning_rate': 0.00021819474058280028, 'step': 2200, 'epoch': 2.35}\n",
      " 82%|███████████████████████████████▊       | 2298/2814 [00:44<00:06, 77.17it/s]{'loss': 0.0373785044712713, 'learning_rate': 0.00018265813788201848, 'step': 2300, 'epoch': 2.45}\n",
      " 85%|█████████████████████████████████▏     | 2395/2814 [00:45<00:05, 73.22it/s]{'loss': 0.0344381243805401, 'learning_rate': 0.00014712153518123668, 'step': 2400, 'epoch': 2.56}\n",
      " 89%|██████████████████████████████████▌    | 2493/2814 [00:47<00:04, 76.18it/s]{'loss': 0.03708043361082673, 'learning_rate': 0.00011158493248045487, 'step': 2500, 'epoch': 2.67}\n",
      " 92%|███████████████████████████████████▉   | 2596/2814 [00:48<00:03, 62.58it/s]{'loss': 0.03763686926336959, 'learning_rate': 7.604832977967307e-05, 'step': 2600, 'epoch': 2.77}\n",
      " 96%|█████████████████████████████████████▍ | 2700/2814 [00:50<00:01, 70.95it/s]{'loss': 0.03803194257896394, 'learning_rate': 4.051172707889126e-05, 'step': 2700, 'epoch': 2.88}\n",
      " 99%|██████████████████████████████████████▋| 2794/2814 [00:51<00:00, 73.76it/s]{'loss': 0.0263264422462089, 'learning_rate': 4.975124378109453e-06, 'step': 2800, 'epoch': 2.99}\n",
      "100%|██████████████████████████████████████▉| 2811/2814 [00:51<00:00, 72.01it/s]***** Running evaluation *****\n",
      "  Num examples = 1250\n",
      "  Batch size = 8\n",
      "{'eval_acc': 0.9832, 'step': 2814, 'epoch': 3.0}\n",
      "Saving model checkpoint to ./trainer_output/checkpoint-2814\n",
      "100%|███████████████████████████████████████| 2814/2814 [00:57<00:00, 49.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_acc': 0.9832}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer1.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c54ba88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "args2 = trainer.SimpleTrainingArguments(\n",
    "    output_dir = './trainer_output',\n",
    "    eval_steps = 1000,\n",
    "    max_steps = 2000,\n",
    "    learning_rate = 1e-2,\n",
    "    per_device_train_batch_size = 64\n",
    ")\n",
    "trainer2 = trainer.BasicTrainer(\n",
    "    model,\n",
    "    args = args2,\n",
    "    train_dataset = tr_ds,\n",
    "    eval_dataset = te_ds,\n",
    "    compute_metrics = compute_accuracy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8715f128",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Total optimization steps = 2000\n",
      "  0%|                                                  | 0/2000 [00:00<?, ?it/s]/home/shuirh/lib/anaconda3/envs/st/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "  5%|█▉                                       | 95/2000 [00:02<00:22, 84.80it/s]{'loss': 0.6270670919120311, 'learning_rate': 0.00095, 'step': 100, 'epoch': 0.11}\n",
      " 10%|███▉                                    | 198/2000 [00:03<00:20, 86.48it/s]{'loss': 0.32939486429095266, 'learning_rate': 0.0009000000000000001, 'step': 200, 'epoch': 0.21}\n",
      " 15%|█████▉                                  | 294/2000 [00:04<00:21, 80.11it/s]{'loss': 0.24090489789843558, 'learning_rate': 0.00085, 'step': 300, 'epoch': 0.32}\n",
      " 20%|███████▉                                | 398/2000 [00:06<00:24, 65.05it/s]{'loss': 0.19022969640791415, 'learning_rate': 0.0008, 'step': 400, 'epoch': 0.43}\n",
      " 25%|█████████▉                              | 498/2000 [00:08<00:23, 64.12it/s]{'loss': 0.180901157297194, 'learning_rate': 0.00075, 'step': 500, 'epoch': 0.53}\n",
      " 30%|███████████▉                            | 597/2000 [00:09<00:17, 81.78it/s]{'loss': 0.16134409299120306, 'learning_rate': 0.0007, 'step': 600, 'epoch': 0.64}\n",
      " 35%|█████████████▉                          | 699/2000 [00:11<00:23, 55.73it/s]{'loss': 0.14963398499414324, 'learning_rate': 0.0006500000000000001, 'step': 700, 'epoch': 0.75}\n",
      " 40%|███████████████▉                        | 797/2000 [00:12<00:19, 62.94it/s]{'loss': 0.15329706218093633, 'learning_rate': 0.0006, 'step': 800, 'epoch': 0.85}\n",
      " 45%|█████████████████▉                      | 899/2000 [00:14<00:17, 64.38it/s]{'loss': 0.10975345195271075, 'learning_rate': 0.00055, 'step': 900, 'epoch': 0.96}\n",
      " 50%|███████████████████▉                    | 998/2000 [00:15<00:13, 71.83it/s]{'loss': 0.09435329080559313, 'learning_rate': 0.0005, 'step': 1000, 'epoch': 1.07}\n",
      "***** Running evaluation *****\n",
      "  Num examples = 1250\n",
      "  Batch size = 8\n",
      "{'eval_acc': 0.9709, 'step': 1000, 'epoch': 1.07}\n",
      "Saving model checkpoint to ./trainer_output/checkpoint-1000\n",
      " 55%|█████████████████████▎                 | 1095/2000 [00:22<00:14, 63.97it/s]{'loss': 0.09662153899669647, 'learning_rate': 0.00045000000000000004, 'step': 1100, 'epoch': 1.17}\n",
      " 59%|███████████████████████▏               | 1189/2000 [00:23<00:09, 88.40it/s]{'loss': 0.08120107727125286, 'learning_rate': 0.0004, 'step': 1200, 'epoch': 1.28}\n",
      " 65%|█████████████████████████▎             | 1295/2000 [00:24<00:08, 83.60it/s]{'loss': 0.07577753939200192, 'learning_rate': 0.00035, 'step': 1300, 'epoch': 1.39}\n",
      " 70%|███████████████████████████▏           | 1394/2000 [00:25<00:08, 75.12it/s]{'loss': 0.07154170909896493, 'learning_rate': 0.0003, 'step': 1400, 'epoch': 1.49}\n",
      " 75%|█████████████████████████████▏         | 1499/2000 [00:27<00:06, 74.12it/s]{'loss': 0.06771100167650729, 'learning_rate': 0.00025, 'step': 1500, 'epoch': 1.6}\n",
      " 80%|███████████████████████████████▏       | 1598/2000 [00:28<00:04, 82.44it/s]{'loss': 0.0692114143865183, 'learning_rate': 0.0002, 'step': 1600, 'epoch': 1.71}\n",
      " 85%|█████████████████████████████████      | 1695/2000 [00:30<00:04, 67.93it/s]{'loss': 0.06813328053336591, 'learning_rate': 0.00015, 'step': 1700, 'epoch': 1.81}\n",
      " 90%|██████████████████████████████████▉    | 1794/2000 [00:31<00:03, 68.12it/s]{'loss': 0.06172158243134618, 'learning_rate': 0.0001, 'step': 1800, 'epoch': 1.92}\n",
      " 95%|████████████████████████████████████▉  | 1894/2000 [00:32<00:01, 81.43it/s]{'loss': 0.05116195886163041, 'learning_rate': 5e-05, 'step': 1900, 'epoch': 2.03}\n",
      "100%|██████████████████████████████████████▉| 1998/2000 [00:34<00:00, 67.54it/s]{'loss': 0.04093311028322205, 'learning_rate': 0.0, 'step': 2000, 'epoch': 2.13}\n",
      "***** Running evaluation *****\n",
      "  Num examples = 1250\n",
      "  Batch size = 8\n",
      "{'eval_acc': 0.9793, 'step': 2000, 'epoch': 2.13}\n",
      "Saving model checkpoint to ./trainer_output/checkpoint-2000\n",
      "Save best model to ./trainer_output/best_model.bin\n",
      "100%|███████████████████████████████████████| 2000/2000 [00:38<00:00, 51.38it/s]\n"
     ]
    }
   ],
   "source": [
    "trainer2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "087627f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "args3 = trainer.SimpleTrainingArguments(\n",
    "    eval_steps = 1000,\n",
    "    max_steps = 2000,\n",
    "    learning_rate = 1e-2,\n",
    "    per_device_train_batch_size = 64\n",
    ")\n",
    "trainer3 = trainer.BasicTrainer(\n",
    "    model,\n",
    "    args = args3,\n",
    "    train_dataset = tr_ds,\n",
    "    eval_dataset = te_ds,\n",
    "    compute_metrics = compute_accuracy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe8f6069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Total optimization steps = 2000\n",
      "  5%|█▉                                       | 94/2000 [00:01<00:30, 62.00it/s]{'loss': 0.07370475668925792, 'learning_rate': 0.00095, 'step': 100, 'epoch': 0.11}\n",
      " 10%|███▉                                    | 198/2000 [00:02<00:20, 88.78it/s]{'loss': 0.08779692580457776, 'learning_rate': 0.0009000000000000001, 'step': 200, 'epoch': 0.21}\n",
      " 14%|█████▊                                  | 290/2000 [00:03<00:21, 78.30it/s]{'loss': 0.07774433095939458, 'learning_rate': 0.00085, 'step': 300, 'epoch': 0.32}\n",
      " 20%|███████▉                                | 394/2000 [00:05<00:20, 78.42it/s]{'loss': 0.07097152324859053, 'learning_rate': 0.0008, 'step': 400, 'epoch': 0.43}\n",
      " 25%|█████████▉                              | 495/2000 [00:06<00:21, 70.44it/s]{'loss': 0.07783148474991322, 'learning_rate': 0.00075, 'step': 500, 'epoch': 0.53}\n",
      " 30%|███████████▉                            | 599/2000 [00:08<00:18, 77.26it/s]{'loss': 0.07682510792044922, 'learning_rate': 0.0007, 'step': 600, 'epoch': 0.64}\n",
      " 35%|██████████████                          | 700/2000 [00:09<00:22, 57.24it/s]{'loss': 0.07966841863002629, 'learning_rate': 0.0006500000000000001, 'step': 700, 'epoch': 0.75}\n",
      " 40%|███████████████▉                        | 797/2000 [00:11<00:18, 64.37it/s]{'loss': 0.07814442325383425, 'learning_rate': 0.0006, 'step': 800, 'epoch': 0.85}\n",
      " 45%|█████████████████▉                      | 896/2000 [00:12<00:15, 69.25it/s]{'loss': 0.05843392542563379, 'learning_rate': 0.00055, 'step': 900, 'epoch': 0.96}\n",
      " 50%|███████████████████▉                    | 999/2000 [00:13<00:12, 77.04it/s]{'loss': 0.04229187134507811, 'learning_rate': 0.0005, 'step': 1000, 'epoch': 1.07}\n",
      "***** Running evaluation *****\n",
      "  Num examples = 1250\n",
      "  Batch size = 8\n",
      "{'eval_acc': 0.9781, 'step': 1000, 'epoch': 1.07}\n",
      " 55%|█████████████████████▎                 | 1092/2000 [00:20<00:18, 49.46it/s]{'loss': 0.03927791559603065, 'learning_rate': 0.00045000000000000004, 'step': 1100, 'epoch': 1.17}\n",
      " 60%|███████████████████████▍               | 1199/2000 [00:22<00:11, 68.34it/s]{'loss': 0.030176806510426105, 'learning_rate': 0.0004, 'step': 1200, 'epoch': 1.28}\n",
      " 65%|█████████████████████████▎             | 1299/2000 [00:23<00:10, 65.82it/s]{'loss': 0.02970973629533546, 'learning_rate': 0.00035, 'step': 1300, 'epoch': 1.39}\n",
      " 70%|███████████████████████████▏           | 1395/2000 [00:25<00:08, 73.28it/s]{'loss': 0.03362643456668593, 'learning_rate': 0.0003, 'step': 1400, 'epoch': 1.49}\n",
      " 75%|█████████████████████████████▎         | 1500/2000 [00:26<00:06, 72.18it/s]{'loss': 0.029927857982111162, 'learning_rate': 0.00025, 'step': 1500, 'epoch': 1.6}\n",
      " 80%|███████████████████████████████▏       | 1597/2000 [00:27<00:06, 63.29it/s]{'loss': 0.03156206675252179, 'learning_rate': 0.0002, 'step': 1600, 'epoch': 1.71}\n",
      " 85%|█████████████████████████████████      | 1696/2000 [00:29<00:04, 67.75it/s]{'loss': 0.0333659310702933, 'learning_rate': 0.00015, 'step': 1700, 'epoch': 1.81}\n",
      " 90%|██████████████████████████████████▉    | 1794/2000 [00:30<00:02, 88.23it/s]{'loss': 0.02996805790811777, 'learning_rate': 0.0001, 'step': 1800, 'epoch': 1.92}\n",
      " 95%|████████████████████████████████████▉  | 1894/2000 [00:32<00:01, 61.83it/s]{'loss': 0.024002540655201302, 'learning_rate': 5e-05, 'step': 1900, 'epoch': 2.03}\n",
      "100%|██████████████████████████████████████▉| 1996/2000 [00:33<00:00, 74.68it/s]{'loss': 0.017086673446465285, 'learning_rate': 0.0, 'step': 2000, 'epoch': 2.13}\n",
      "***** Running evaluation *****\n",
      "  Num examples = 1250\n",
      "  Batch size = 8\n",
      "{'eval_acc': 0.9845, 'step': 2000, 'epoch': 2.13}\n",
      "100%|███████████████████████████████████████| 2000/2000 [00:39<00:00, 50.61it/s]\n"
     ]
    }
   ],
   "source": [
    "trainer3.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b0895c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d55ba9d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'num_class'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_115372/3418168331.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-chinese'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m21\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/lib/anaconda3/envs/st/lib/python3.8/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1323\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mno_init_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_enable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_fast_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1325\u001b[0;31m                 \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfrom_pt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'num_class'"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-chinese', num_class = 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2f678446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer3.lr_scheduler.get_last_lr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc646ce0",
   "metadata": {},
   "source": [
    "## Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "105e4936",
   "metadata": {},
   "outputs": [],
   "source": [
    "import modules\n",
    "from transformers.data.data_collator import default_data_collator\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2775611d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8f1102ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/shuirh/project/research/invariant_learning/rnp/cache/cail_transformer_ds.pkl', 'rb') as f:\n",
    "    train,dev,test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "73d8ebd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.set_label_name('labels_charge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9dc4769",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train, batch_size = 8, collate_fn = default_data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52dd9395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: 518\n",
      "attention_mask: 518\n",
      "token_type_ids: 518\n",
      "labels: 0\n"
     ]
    }
   ],
   "source": [
    "for k,v in train[2].items():\n",
    "    print(f\"{k}: {len(v) if isinstance(v, list) else 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b648c172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b107394f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6667)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.LongTensor([1,2,3]) == torch.LongTensor([1,2,0])).float().mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:st]",
   "language": "python",
   "name": "conda-env-st-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
